{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47df03bc",
   "metadata": {},
   "source": [
    "## Thejas Balenahalli Kiran\n",
    "## December 06, 2021\n",
    "# <center>Predicting eBay Package Delivery Date</center>\n",
    "## <center>Final Project STAT 5000</center>\n",
    "### Project Link: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873544e3",
   "metadata": {},
   "source": [
    "# ABSTRACT\n",
    "### Keywords\n",
    "Machine Learning, Multi-variate Linar Regression, MLP Regressor, XGBoost, Delivery Date Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfeae3",
   "metadata": {},
   "source": [
    "<div style=\"page-break: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194835a7",
   "metadata": {},
   "source": [
    "# Project Introduction\n",
    "\n",
    "### Acknowledgement\n",
    "I would like to thank eBay for providing me an opportunity to work with a real-time dataset by hosting a ML Challenge on eval.AI\n",
    "\n",
    "### Problem Statement\n",
    "To predict the delivery dates of packages sold on eBay by both customers and businesses.\n",
    "\n",
    "### Recommended Hardware Specifications\n",
    "1. Intel i5 Processor\n",
    "2. 16GB RAM\n",
    "3. 10GB Hard Disk Space\n",
    "\n",
    "### Recommended Software Specifications\n",
    "1. Python 3.7.1\n",
    "2. Jupyter Notebook\n",
    "\n",
    "### Data Source\n",
    "The data for predicting the eBay delivery dates were given to me by eBay as a part of their ML Challenge 2021. I have used the same data for my models and as per their policy agreements, I do not have the authority to share the data with others but will show a snippet while working through the whole process. However, I will be happy to show the data in-person if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a4ac4d",
   "metadata": {},
   "source": [
    "# Literature Review\n",
    "\n",
    "1. \"Predicting Shipping Time with Machine Learning\", a project presented at Massachusets Institute of Technology in the year 2012 by Antonie Charles Jean Jonquais and Florian Krempl focussed on predicting delivery times of freights for Maersk (Shipping company in Denmark. Although they had an in-house way of predicting the delivery dates, they wanted to build a better and more accurate model using Machine Learning and predictive analytics. They implemented the random forest algorithm and found it to be surprisingly better than neural networks in terms of accuracy and prediction time. \n",
    "\n",
    "2. An article written by Purdue University focuses on estimating delivery time for industrial pieces of equipment using different machine learning approaches. Some of the approaches they implemented include Support Vector Machines, Random Forest, k-Nearest Neighbors, XGBoost among others. After analysis and further research, they found the Random Forest and SVM Ensemble models to be the best with high accuracies and low Mean Squared Squared Errors. They plan to enhance the model further by adding features that weren't collected for this phase of the research. \n",
    "\n",
    "3. The paper written by Fan Wu et. al. was presented at the Association for the Advancement of Artificial Intelligence under the title \"DeepETA: A Spatial-Temporal Sequential Neural Network Model for Estimating Time of Arrival in Package Delivery System\". The paper considers the features which could not be included in the traditional models i.e. frequency of delivery routes, taking multiple destinations on the same route into consideration. Including the above-mentioned and additional features, they have developed a recurrent neural network model for predicting the delivery dates in China.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b683403c",
   "metadata": {},
   "source": [
    "# Understanding the dataset\n",
    "The dataset given to us by eBay consists of 15,000,000 data points with each data point being explained by 19 attributes. The attributes that define each datapoint are,\n",
    "1. <u>b2c_c2c</u> - It of type string with values b2c or c2c. This explains if the transaction happening is between business to customers or between customers and other customers.\n",
    "2. <u>seller_id</u> - This is a unique ID given to each seller for identification. It is of type Long Int.\n",
    "3. <u>declared_handling_days</u> - The number of days taken by the seller to ship the carrier from the day of acceptance.\n",
    "4. <u>acceptance_scan_timestamp</u> - The date and time when the carrier has accepted the package for the final shipment. The values in this are of type timestamp.\n",
    "5. <u>shipment_method_id</u> - The integer type attribute defines the type of shipping service declared by the seller.\n",
    "6. <u>shipping_fee</u> - Transportation and handling charges charged by the seller for shipping the items. All the values are in USD.\n",
    "7. <u>carrier_min_estimate</u> - The minimum estimate of the number of required days by the carrier for the specified service.\n",
    "8. <u>carrier_max_estimate</u> - The maximum estimate of the number of required days by the carrier for the specified service.\n",
    "9. <u>item_zip</u> - The US Postal zip code of the package origin/source.\n",
    "10. <u>buyer_zip</u> - The US Postal zip code of the package destination.\n",
    "11. <u>category_id</u> - An integer type data attribute that categorizes the package by its type.\n",
    "12. <u>item_price</u> - The price per item involved in the transaction.\n",
    "13. <u>quantity</u> - Number of items involved in the transaction.\n",
    "14. <u>payment_datetime</u> - A timestamp attribute that clocks in the time when the payment has been done for the particular transaction.\n",
    "15. <u>delivery_date</u> - The actual delivery date of the package. This is the attribute that we need to predict using the other attributes.\n",
    "16. <u>weight</u> - A scalar value that determines the sum of the weight of all quantities involved in the transaction.\n",
    "17. <u>weight_units</u> - It defines the weight scalar value by telling if the measurements are in kilograms or pounds. Pounds are represented as 1 and Kilograms are denoted by 2.\n",
    "18. <u>package_size</u> - It is a categorical value which categorizes the packages based on its sizes.\n",
    "19. <u>record_number</u> - Unique integer number given to the transaction to identify them.\n",
    "\n",
    "Before going ahead to implement the model after understanding the data, I found some irregularities in the dataset and have cleaned them before moving ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47896e",
   "metadata": {},
   "source": [
    "# Importing the libraries and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db173920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from uszipcode import SearchEngine\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08365f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b2c_c2c</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>declared_handling_days</th>\n",
       "      <th>acceptance_scan_timestamp</th>\n",
       "      <th>shipment_method_id</th>\n",
       "      <th>shipping_fee</th>\n",
       "      <th>carrier_min_estimate</th>\n",
       "      <th>carrier_max_estimate</th>\n",
       "      <th>item_zip</th>\n",
       "      <th>buyer_zip</th>\n",
       "      <th>category_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>payment_datetime</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>weight</th>\n",
       "      <th>weight_units</th>\n",
       "      <th>package_size</th>\n",
       "      <th>record_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B2C</td>\n",
       "      <td>25454</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2019-03-26 15:11:00.000-07:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>97219</td>\n",
       "      <td>49040</td>\n",
       "      <td>13</td>\n",
       "      <td>27.95</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-03-24 03:56:49.000-07:00</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>LETTER</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C2C</td>\n",
       "      <td>6727381</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-06-02 12:53:00.000-07:00</td>\n",
       "      <td>0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>11415-3528</td>\n",
       "      <td>62521</td>\n",
       "      <td>0</td>\n",
       "      <td>20.50</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-01 13:43:54.000-07:00</td>\n",
       "      <td>2018-06-05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B2C</td>\n",
       "      <td>18507</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-07 16:22:00.000-05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>27292</td>\n",
       "      <td>53010</td>\n",
       "      <td>1</td>\n",
       "      <td>19.90</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-06 00:02:00.000-05:00</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B2C</td>\n",
       "      <td>4677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-12-17 16:56:00.000-08:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>90703</td>\n",
       "      <td>80022</td>\n",
       "      <td>1</td>\n",
       "      <td>35.50</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-16 10:28:28.000-08:00</td>\n",
       "      <td>2018-12-21</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B2C</td>\n",
       "      <td>4677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-07-27 16:48:00.000-07:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>90703</td>\n",
       "      <td>55070</td>\n",
       "      <td>1</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-07-26 18:20:02.000-07:00</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B2C</td>\n",
       "      <td>10514</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-04-19 19:42:00.000-04:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>43215</td>\n",
       "      <td>77063</td>\n",
       "      <td>3</td>\n",
       "      <td>10.39</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-18 14:11:09.000-04:00</td>\n",
       "      <td>2019-04-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B2C</td>\n",
       "      <td>104</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-02-08 17:35:00.000-08:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>91304</td>\n",
       "      <td>60565</td>\n",
       "      <td>11</td>\n",
       "      <td>5.70</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-08 09:33:13.000-08:00</td>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B2C</td>\n",
       "      <td>340356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-04-23 17:31:00.000-04:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>49735</td>\n",
       "      <td>29379</td>\n",
       "      <td>1</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-04-22 18:32:04.000-04:00</td>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B2C</td>\n",
       "      <td>113915</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2019-10-12 09:22:00.000-04:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>43606</td>\n",
       "      <td>32958</td>\n",
       "      <td>18</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-11 04:54:25.000-04:00</td>\n",
       "      <td>2019-10-15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NONE</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B2C</td>\n",
       "      <td>130301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-08-09 11:24:00.000-05:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>35117</td>\n",
       "      <td>84776</td>\n",
       "      <td>13</td>\n",
       "      <td>59.98</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-08-08 12:47:14.000-05:00</td>\n",
       "      <td>2019-08-12</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B2C</td>\n",
       "      <td>206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-04-02 19:42:00.000-07:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>93309</td>\n",
       "      <td>20774</td>\n",
       "      <td>12</td>\n",
       "      <td>9.64</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-03-31 06:58:25.000-07:00</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B2C</td>\n",
       "      <td>2101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-05-08 15:26:00.000-05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>51031</td>\n",
       "      <td>28092</td>\n",
       "      <td>13</td>\n",
       "      <td>9.90</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-05-07 11:40:27.000-05:00</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B2C</td>\n",
       "      <td>12924</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-08-06 16:23:00.000-05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>77035</td>\n",
       "      <td>45373</td>\n",
       "      <td>8</td>\n",
       "      <td>23.99</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-06 13:04:18.000-05:00</td>\n",
       "      <td>2018-08-09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C2C</td>\n",
       "      <td>487782</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-12-08 18:17:00.000-08:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>91789</td>\n",
       "      <td>27317</td>\n",
       "      <td>11</td>\n",
       "      <td>15.88</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-02 18:31:17.000-08:00</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>B2C</td>\n",
       "      <td>5815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-05-21 19:50:14.000-04:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>32605</td>\n",
       "      <td>33166</td>\n",
       "      <td>11</td>\n",
       "      <td>22.78</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-05-20 18:23:31.000-04:00</td>\n",
       "      <td>2019-05-22</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B2C</td>\n",
       "      <td>17757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-15 16:39:00.000-04:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>04268</td>\n",
       "      <td>65459</td>\n",
       "      <td>13</td>\n",
       "      <td>29.99</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-14 19:04:18.000-04:00</td>\n",
       "      <td>2019-04-19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>LETTER</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B2C</td>\n",
       "      <td>48312</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-02-12 23:14:00.000-05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>20743</td>\n",
       "      <td>95122</td>\n",
       "      <td>0</td>\n",
       "      <td>22.99</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-10 19:37:32.000-05:00</td>\n",
       "      <td>2018-02-14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>B2C</td>\n",
       "      <td>2077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-07-18 14:57:00.000-05:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>77471</td>\n",
       "      <td>55420</td>\n",
       "      <td>5</td>\n",
       "      <td>65.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-07-17 13:42:15.000-05:00</td>\n",
       "      <td>2018-07-20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NONE</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B2C</td>\n",
       "      <td>1083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-12-17 18:17:00.000-07:00</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>83642</td>\n",
       "      <td>77488</td>\n",
       "      <td>1</td>\n",
       "      <td>129.99</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-15 22:00:07.000-07:00</td>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>B2C</td>\n",
       "      <td>55903</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-09-25 15:20:00.000-04:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>08215</td>\n",
       "      <td>15522</td>\n",
       "      <td>1</td>\n",
       "      <td>195.99</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-09-25 10:52:54.000-04:00</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>C2C</td>\n",
       "      <td>166588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-03-26 21:03:00.000-07:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>95928</td>\n",
       "      <td>95973</td>\n",
       "      <td>10</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-24 15:59:16.000-07:00</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>B2C</td>\n",
       "      <td>70189</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-08-29 15:35:00.000-04:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>32119</td>\n",
       "      <td>83221</td>\n",
       "      <td>8</td>\n",
       "      <td>16.49</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-27 01:00:48.000-04:00</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>B2C</td>\n",
       "      <td>10798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-05-08 14:03:00.000-07:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>92532</td>\n",
       "      <td>12508</td>\n",
       "      <td>8</td>\n",
       "      <td>7.29</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-05-07 16:15:10.000-07:00</td>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>B2C</td>\n",
       "      <td>16000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-04-09 15:57:00.000-07:00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>97240</td>\n",
       "      <td>80107</td>\n",
       "      <td>10</td>\n",
       "      <td>19.96</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-04-05 17:08:25.000-07:00</td>\n",
       "      <td>2018-04-12</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>B2C</td>\n",
       "      <td>16000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-04-14 18:14:00.000-07:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>97240</td>\n",
       "      <td>67147</td>\n",
       "      <td>10</td>\n",
       "      <td>19.96</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-04-12 09:06:20.000-07:00</td>\n",
       "      <td>2018-04-16</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>PACKAGE_THICK_ENVELOPE</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   b2c_c2c  seller_id  declared_handling_days      acceptance_scan_timestamp  \\\n",
       "0      B2C      25454                     3.0  2019-03-26 15:11:00.000-07:00   \n",
       "1      C2C    6727381                     2.0  2018-06-02 12:53:00.000-07:00   \n",
       "2      B2C      18507                     1.0  2019-01-07 16:22:00.000-05:00   \n",
       "3      B2C       4677                     1.0  2018-12-17 16:56:00.000-08:00   \n",
       "4      B2C       4677                     1.0  2018-07-27 16:48:00.000-07:00   \n",
       "5      B2C      10514                     1.0  2019-04-19 19:42:00.000-04:00   \n",
       "6      B2C        104                     1.0  2019-02-08 17:35:00.000-08:00   \n",
       "7      B2C     340356                     1.0  2018-04-23 17:31:00.000-04:00   \n",
       "8      B2C     113915                     5.0  2019-10-12 09:22:00.000-04:00   \n",
       "9      B2C     130301                     1.0  2019-08-09 11:24:00.000-05:00   \n",
       "10     B2C        206                     1.0  2019-04-02 19:42:00.000-07:00   \n",
       "11     B2C       2101                     1.0  2018-05-08 15:26:00.000-05:00   \n",
       "12     B2C      12924                     1.0  2018-08-06 16:23:00.000-05:00   \n",
       "13     C2C     487782                     2.0  2018-12-08 18:17:00.000-08:00   \n",
       "14     B2C       5815                     0.0  2019-05-21 19:50:14.000-04:00   \n",
       "15     B2C      17757                     0.0  2019-04-15 16:39:00.000-04:00   \n",
       "16     B2C      48312                     1.0  2018-02-12 23:14:00.000-05:00   \n",
       "17     B2C       2077                     1.0  2018-07-18 14:57:00.000-05:00   \n",
       "18     B2C       1083                     1.0  2018-12-17 18:17:00.000-07:00   \n",
       "19     B2C      55903                     2.0  2019-09-25 15:20:00.000-04:00   \n",
       "20     C2C     166588                     1.0  2018-03-26 21:03:00.000-07:00   \n",
       "21     B2C      70189                     3.0  2018-08-29 15:35:00.000-04:00   \n",
       "22     B2C      10798                     0.0  2018-05-08 14:03:00.000-07:00   \n",
       "23     B2C      16000                     3.0  2018-04-09 15:57:00.000-07:00   \n",
       "24     B2C      16000                     3.0  2018-04-14 18:14:00.000-07:00   \n",
       "\n",
       "    shipment_method_id  shipping_fee  carrier_min_estimate  \\\n",
       "0                    0          0.00                     3   \n",
       "1                    0          3.00                     3   \n",
       "2                    0          4.50                     3   \n",
       "3                    0          0.00                     3   \n",
       "4                    0          0.00                     3   \n",
       "5                    0          0.00                     3   \n",
       "6                    0          0.00                     3   \n",
       "7                    0          2.95                     3   \n",
       "8                    3          0.00                     2   \n",
       "9                    1          0.00                     2   \n",
       "10                   0          0.00                     3   \n",
       "11                   0          0.00                     3   \n",
       "12                   0          0.00                     3   \n",
       "13                   0          0.00                     3   \n",
       "14                   0          0.00                     3   \n",
       "15                   0          0.00                     3   \n",
       "16                   0          4.00                     3   \n",
       "17                   1          0.00                     2   \n",
       "18                   5          0.00                     2   \n",
       "19                   2          0.00                     2   \n",
       "20                   0          0.00                     3   \n",
       "21                   0          0.00                     3   \n",
       "22                   0          0.00                     3   \n",
       "23                   6          0.00                     2   \n",
       "24                   6          2.75                     2   \n",
       "\n",
       "    carrier_max_estimate    item_zip buyer_zip  category_id  item_price  \\\n",
       "0                      5       97219     49040           13       27.95   \n",
       "1                      5  11415-3528     62521            0       20.50   \n",
       "2                      5       27292     53010            1       19.90   \n",
       "3                      5       90703     80022            1       35.50   \n",
       "4                      5       90703     55070            1       25.00   \n",
       "5                      5       43215     77063            3       10.39   \n",
       "6                      5       91304     60565           11        5.70   \n",
       "7                      5       49735     29379            1        6.00   \n",
       "8                      8       43606     32958           18        5.55   \n",
       "9                      5       35117     84776           13       59.98   \n",
       "10                     5       93309     20774           12        9.64   \n",
       "11                     5       51031     28092           13        9.90   \n",
       "12                     5       77035     45373            8       23.99   \n",
       "13                     5       91789     27317           11       15.88   \n",
       "14                     5       32605     33166           11       22.78   \n",
       "15                     5       04268     65459           13       29.99   \n",
       "16                     5       20743     95122            0       22.99   \n",
       "17                     5       77471     55420            5       65.00   \n",
       "18                     5       83642     77488            1      129.99   \n",
       "19                     9       08215     15522            1      195.99   \n",
       "20                     5       95928     95973           10        1.29   \n",
       "21                     5       32119     83221            8       16.49   \n",
       "22                     5       92532     12508            8        7.29   \n",
       "23                     5       97240     80107           10       19.96   \n",
       "24                     5       97240     67147           10       19.96   \n",
       "\n",
       "    quantity               payment_datetime delivery_date  weight  \\\n",
       "0          1  2019-03-24 03:56:49.000-07:00    2019-03-29       5   \n",
       "1          1  2018-06-01 13:43:54.000-07:00    2018-06-05       0   \n",
       "2          1  2019-01-06 00:02:00.000-05:00    2019-01-10       9   \n",
       "3          1  2018-12-16 10:28:28.000-08:00    2018-12-21       8   \n",
       "4          1  2018-07-26 18:20:02.000-07:00    2018-07-30       3   \n",
       "5          1  2019-04-18 14:11:09.000-04:00    2019-04-22       1   \n",
       "6          1  2019-02-08 09:33:13.000-08:00    2019-02-11       0   \n",
       "7          1  2018-04-22 18:32:04.000-04:00    2018-04-25       1   \n",
       "8          1  2019-10-11 04:54:25.000-04:00    2019-10-15       0   \n",
       "9          1  2019-08-08 12:47:14.000-05:00    2019-08-12     112   \n",
       "10         1  2019-03-31 06:58:25.000-07:00    2019-04-04       7   \n",
       "11         1  2018-05-07 11:40:27.000-05:00    2018-05-14       2   \n",
       "12         1  2018-08-06 13:04:18.000-05:00    2018-08-09       1   \n",
       "13         1  2018-12-02 18:31:17.000-08:00    2018-12-12       0   \n",
       "14         2  2019-05-20 18:23:31.000-04:00    2019-05-22       8   \n",
       "15         1  2019-04-14 19:04:18.000-04:00    2019-04-19       0   \n",
       "16         1  2018-02-10 19:37:32.000-05:00    2018-02-14       6   \n",
       "17         1  2018-07-17 13:42:15.000-05:00    2018-07-20       0   \n",
       "18         1  2018-12-15 22:00:07.000-07:00    2018-12-20       6   \n",
       "19         1  2019-09-25 10:52:54.000-04:00    2019-09-27       0   \n",
       "20         1  2018-03-24 15:59:16.000-07:00    2018-03-27       0   \n",
       "21         1  2018-08-27 01:00:48.000-04:00    2018-09-01       0   \n",
       "22         1  2018-05-07 16:15:10.000-07:00    2018-05-11       0   \n",
       "23         1  2018-04-05 17:08:25.000-07:00    2018-04-12      19   \n",
       "24         1  2018-04-12 09:06:20.000-07:00    2018-04-16      19   \n",
       "\n",
       "    weight_units            package_size  record_number  \n",
       "0              1                  LETTER              1  \n",
       "1              1  PACKAGE_THICK_ENVELOPE              2  \n",
       "2              1  PACKAGE_THICK_ENVELOPE              3  \n",
       "3              1  PACKAGE_THICK_ENVELOPE              4  \n",
       "4              1  PACKAGE_THICK_ENVELOPE              5  \n",
       "5              1  PACKAGE_THICK_ENVELOPE              6  \n",
       "6              1  PACKAGE_THICK_ENVELOPE              7  \n",
       "7              1  PACKAGE_THICK_ENVELOPE              8  \n",
       "8              1                    NONE              9  \n",
       "9              1  PACKAGE_THICK_ENVELOPE             10  \n",
       "10             1  PACKAGE_THICK_ENVELOPE             11  \n",
       "11             1  PACKAGE_THICK_ENVELOPE             12  \n",
       "12             1  PACKAGE_THICK_ENVELOPE             13  \n",
       "13             1  PACKAGE_THICK_ENVELOPE             14  \n",
       "14             1  PACKAGE_THICK_ENVELOPE             15  \n",
       "15             1                  LETTER             16  \n",
       "16             1  PACKAGE_THICK_ENVELOPE             17  \n",
       "17             1                    NONE             18  \n",
       "18             1  PACKAGE_THICK_ENVELOPE             19  \n",
       "19             1  PACKAGE_THICK_ENVELOPE             20  \n",
       "20             1  PACKAGE_THICK_ENVELOPE             21  \n",
       "21             1  PACKAGE_THICK_ENVELOPE             22  \n",
       "22             1  PACKAGE_THICK_ENVELOPE             23  \n",
       "23             1  PACKAGE_THICK_ENVELOPE             24  \n",
       "24             1  PACKAGE_THICK_ENVELOPE             25  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing our data and checking the structure of it\n",
    "data = pd.read_csv('/Users/ujas/Downloads/eBay_ML_Challenge_Dataset_2021/eBay_ML_Challenge_Dataset_2021_train.tsv', \n",
    "                   sep = '\\t')\n",
    "data.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f73e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['b2c_c2c', 'seller_id', 'declared_handling_days',\n",
       "       'acceptance_scan_timestamp', 'shipment_method_id', 'shipping_fee',\n",
       "       'carrier_min_estimate', 'carrier_max_estimate', 'item_zip', 'buyer_zip',\n",
       "       'category_id', 'item_price', 'quantity', 'payment_datetime',\n",
       "       'delivery_date', 'weight', 'weight_units', 'package_size',\n",
       "       'record_number'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The columns in our dataset are\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30bda377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b2c_c2c                       object\n",
       "seller_id                      int64\n",
       "declared_handling_days       float64\n",
       "acceptance_scan_timestamp     object\n",
       "shipment_method_id             int64\n",
       "shipping_fee                 float64\n",
       "carrier_min_estimate           int64\n",
       "carrier_max_estimate           int64\n",
       "item_zip                      object\n",
       "buyer_zip                     object\n",
       "category_id                    int64\n",
       "item_price                   float64\n",
       "quantity                       int64\n",
       "payment_datetime              object\n",
       "delivery_date                 object\n",
       "weight                         int64\n",
       "weight_units                   int64\n",
       "package_size                  object\n",
       "record_number                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the data types of different columns that can help us see if any columns need to be cleaned particularly\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b58783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b2c_c2c                             2\n",
       "seller_id                     1759305\n",
       "declared_handling_days             11\n",
       "acceptance_scan_timestamp     2245193\n",
       "shipment_method_id                 25\n",
       "shipping_fee                     7044\n",
       "carrier_min_estimate                6\n",
       "carrier_max_estimate                6\n",
       "item_zip                        50939\n",
       "buyer_zip                       57273\n",
       "category_id                        33\n",
       "item_price                      41571\n",
       "quantity                          147\n",
       "payment_datetime             14090416\n",
       "delivery_date                     767\n",
       "weight                           1298\n",
       "weight_units                        2\n",
       "package_size                        7\n",
       "record_number                15000000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the number of unique values in each column \n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d61010",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "After understanding the dataset, I have gone through the data to make sure there are no faulty data points. I have spent a lot of time on this as I believe a good data is a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a0d4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b2c_c2c                           0\n",
       "seller_id                         0\n",
       "declared_handling_days       702886\n",
       "acceptance_scan_timestamp         0\n",
       "shipment_method_id                0\n",
       "shipping_fee                      0\n",
       "carrier_min_estimate              0\n",
       "carrier_max_estimate              0\n",
       "item_zip                          1\n",
       "buyer_zip                         1\n",
       "category_id                       0\n",
       "item_price                        0\n",
       "quantity                          0\n",
       "payment_datetime                  0\n",
       "delivery_date                     0\n",
       "weight                            0\n",
       "weight_units                      0\n",
       "package_size                      0\n",
       "record_number                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there are any null values in the dataset\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14e04ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B2C', 'C2C'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since there are no null values in b2c_c2c, I checked the unique values in them\n",
    "data['b2c_c2c'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d829f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the column b2c_c2c to integer values to pass into the model later on\n",
    "data['b2c_c2c'] = data['b2c_c2c'].replace({'B2C':0, 'C2C':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07405218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there are any seller_id's in the negative range and since there aren't any, we'll move ahead with this\n",
    "sum(data['seller_id'] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5be19ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since, there are Null values in the declared_handling_days column, I will be handling them later. I am also checking if there \n",
    "# are any negative values.\n",
    "sum(data['declared_handling_days'] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cab2f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  2.,  1.,  5.,  0., 10., nan,  4., 30., 15., 20., 40.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since declared_handling_days is of type float and has only 11 unique values, I will print them to see if I can have them as a \n",
    "# categorical variable and to check if there are any outliers\n",
    "data['declared_handling_days'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79cf92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are no null values and all the variables in it are of data type timestamp, I will not be doing any cleaning on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78d8e90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9341444\n",
       "1     2955149\n",
       "2      856653\n",
       "3      780997\n",
       "4      297472\n",
       "5      253508\n",
       "6      176007\n",
       "7      137404\n",
       "8      125093\n",
       "9       39639\n",
       "10      27081\n",
       "11       5906\n",
       "13       1545\n",
       "12       1077\n",
       "14        447\n",
       "15        431\n",
       "16         48\n",
       "19         21\n",
       "17         20\n",
       "18         17\n",
       "21         16\n",
       "20         13\n",
       "22          6\n",
       "24          4\n",
       "26          2\n",
       "Name: shipment_method_id, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since shiping_method_id has no Null values, I will be checking for the number of times each shipping method is used. This is \n",
    "# to make sure that there are no outliers in the column\n",
    "data.shipment_method_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b722f881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0's in shipping_fee: 9000100\n",
      "Number of negative's in shipping_fee: 21\n"
     ]
    }
   ],
   "source": [
    "# Since there are no null values, I will be checking for any manual entry faults i.e. if there are any negative or zero values\n",
    "print('Number of 0\\'s in shipping_fee:', sum(data['shipping_fee'] == 0))\n",
    "print('Number of negative\\'s in shipping_fee:', sum(data['shipping_fee'] < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e65d9998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-cb459675baff>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['shipping_fee'][i-1] = None\n"
     ]
    }
   ],
   "source": [
    "# Since, there are a lot of zero values I am assuming this is not an eror. As far as negative values go, there are 21 of them\n",
    "# and I will be putting them to None to remove it later\n",
    "for i in list(data[(data['shipping_fee']<0)]['record_number']):\n",
    "    data['shipping_fee'][i-1] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c100fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095\n",
      "[ 3  2  1  6 -1  0]\n"
     ]
    }
   ],
   "source": [
    "# Since, there are no null values in the carrier_min_estimate, I am checking for any negative value\n",
    "print(sum(data['carrier_min_estimate'] < 0))\n",
    "\n",
    "# Since, there are nearly 2000 negative values, I am looking for what they are and changing them to None for later processing\n",
    "print(data['carrier_min_estimate'].unique())\n",
    "data[\"carrier_min_estimate\"] = data[\"carrier_min_estimate\"].replace({-1:None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d34e836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095\n",
      "[ 5  8  9  1 25 -1]\n"
     ]
    }
   ],
   "source": [
    "# Since, there are no null values in the carrier_max_estimate, I am checking for any negative value\n",
    "print(sum(data['carrier_max_estimate'] < 0))\n",
    "\n",
    "# Since, there are nearly 2000 negative values, I am looking for what they are and changing them to None for later processing\n",
    "print(data['carrier_max_estimate'].unique())\n",
    "data[\"carrier_max_estimate\"] = data[\"carrier_max_estimate\"].replace({-1:None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a79910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since, there is a Null value in item_zip, I will be removing it now and any other faulty data points as I will be calculating \n",
    "# distance between item_zip and buyer_zip before building the model\n",
    "for x in range(len(data['item_zip'])):\n",
    "    try:\n",
    "        x = data['item_zip'][x][0]\n",
    "    except:\n",
    "        to_drop = x\n",
    "        \n",
    "#Removing the Null values\n",
    "data.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaf48a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index values of teh dataframe\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f0badb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I see more specific 9 digits pin code in the dataset, I am slicing them to 5 digits for easier calculations as many of the \n",
    "# zip codes (item_zip and buyer_zip) are of size 5\n",
    "data['item_zip'] = data['item_zip'].str.slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edd0d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since, there is a Null value in buyer_zip, I will be removing it now and any other faulty data points as I will be calculating \n",
    "# distance between item_zip and buyer_zip before building the model\n",
    "for x in range(len(data['buyer_zip'])):\n",
    "    try:\n",
    "        x = data['buyer_zip'][x][0]\n",
    "    except:\n",
    "        to_drop = x\n",
    "        \n",
    "#Removing the Null values\n",
    "data.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "722326c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index values of teh dataframe\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d06ae517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I see more specific 9 digits pin code in the dataset, I am slicing them to 5 digits for easier calculations as many of the \n",
    "# zip codes (item_zip and buyer_zip) are of size 5\n",
    "data['buyer_zip'] = data['buyer_zip'].str.slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7ee8b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0     2544488\n",
      "1     1293227\n",
      "2     1281052\n",
      "3     1233523\n",
      "4      931637\n",
      "5      787817\n",
      "6      772039\n",
      "7      720987\n",
      "8      569544\n",
      "10     543662\n",
      "9      538951\n",
      "11     445998\n",
      "12     434892\n",
      "13     429443\n",
      "14     366025\n",
      "15     320721\n",
      "16     303283\n",
      "17     257026\n",
      "19     239080\n",
      "18     215859\n",
      "20     125426\n",
      "22     102039\n",
      "21      97935\n",
      "23      94685\n",
      "24      69860\n",
      "25      65579\n",
      "26      64628\n",
      "27      39559\n",
      "28      38358\n",
      "29      32863\n",
      "30      28701\n",
      "31       9216\n",
      "32       1895\n",
      "Name: category_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Since, there are no Null values, I check if there are any negative values and for values 0 assuming it is proxy. I find a \n",
    "# lot of 0 values and looking at the distribution I assume it is a part of the categorical variable \n",
    "print(sum(data['category_id'] < 0))\n",
    "print(data.category_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5658c099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Since, there are no Null values in the item_price, we check for prices less than or equal to zero\n",
    "print(sum(data['item_price'] == 0))\n",
    "print(sum(data['item_price'] < 0))\n",
    "# Since, there are no negative values and just 2 values equal to 0, I am not changing anything in the datapoint assuming these\n",
    "# products were given for free on offer or discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6a608b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As there are no Null values in quantity, I am checking if there are any negative values in the column\n",
    "sum(data['quantity'] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5057600b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since, weight is a value which cannot be negative, I am checking if there are any negative faulty data points\n",
    "sum(data['weight'] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08341ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there are two types of weights (Pounds and Kilograms) in the dataset, I will be converting everything to Kilograms\n",
    "def converting_weights(row):\n",
    "    return [row[\"weight\"] if row[\"weight_units\"] == 2 else (row[\"weight\"] * 0.45359237)][0]\n",
    "\n",
    "data[\"weight_kg\"] = data.apply(converting_weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80f61653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4792626\n"
     ]
    }
   ],
   "source": [
    "# Checking if there are weights with 0 values and making them None\n",
    "print(sum(data['weight_kg'] == 0))\n",
    "data[\"weight_kg\"] = data[\"weight_kg\"].replace({0:None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1a04bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PACKAGE_THICK_ENVELOPE    12652643\n",
      "NONE                       1055227\n",
      "LETTER                      912894\n",
      "LARGE_ENVELOPE              209218\n",
      "LARGE_PACKAGE               170014\n",
      "VERY_LARGE_PACKAGE               1\n",
      "EXTRA_LARGE_PACKAGE              1\n",
      "Name: package_size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# To see the frequency of package_size that are being shipped\n",
    "print(data['package_size'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "290c7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the string_type values in the package_size to a unique number\n",
    "data[\"package_size\"] = data[\"package_size\"].replace({'NONE':None,'PACKAGE_THICK_ENVELOPE':0,'LETTER':1,'LARGE_ENVELOPE':2,\\\n",
    "                                                     'LARGE_PACKAGE':3,'VERY_LARGE_PACKAGE':4,'EXTRA_LARGE_PACKAGE':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01bb8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column with the values as teh average of carrier_min_estimate and carrier_max_estimate\n",
    "data = data.assign(carrier_average_estimate = lambda x: (x.carrier_min_estimate + x.carrier_max_estimate) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77bf82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am creating a new column called distance which is an approximate measure of the distance between the item_zip and buer_zip\n",
    "# To create the distance column, I am first collecting the list of latitude and logitude of zip codes that are present in the\n",
    "# dataset into a dctionary\n",
    "\n",
    "search = SearchEngine(simple_zipcode=True)\n",
    "\n",
    "zipcodes_needed = set()\n",
    "for i in data['item_zip']:\n",
    "    zipcodes_needed.add(i)\n",
    "for i in data['buyer_zip']:\n",
    "    zipcodes_needed.add(i)\n",
    "    \n",
    "zipcodes_needed_list = list(zipcodes_needed)\n",
    "zipcodes_dict = {}\n",
    "\n",
    "for i in zipcodes_needed_list:\n",
    "    zip1 = search.by_zipcode(i)\n",
    "    zipcodes_dict[i]=[zip1.lat, zip1.lng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ec623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning each datapoint with new columns that consists of the latitude and logitude of both seller and buyer from the \n",
    "# previously created dictionary\n",
    "\n",
    "def itemlatlong(series):\n",
    "    lat = zipcodes_dict[series['item_zip']]\n",
    "    return lat\n",
    "data['item_coord'] = data.apply(itemlatlong, axis = 1)\n",
    "\n",
    "def buyerlatlong(series):\n",
    "    lat = zipcodes_dict[series['buyer_zip']]\n",
    "    return lat\n",
    "data['buyer_coord'] = data.apply(buyerlatlong, axis = 1)\n",
    "\n",
    "data[['item_lat','item_long']] = pd.DataFrame(data.item_coord.tolist(), index= data.index)\n",
    "data[['buyer_lat','buyer_long']] = pd.DataFrame(data.buyer_coord.tolist(), index= data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using these latitudes and logitudes, I am caculating the crow distance between these 2 geo location points using the haversine\n",
    "# formula. The crow distance is considered because I do not know the type of transportation used i.e. freights, trucks, \n",
    "# airplanes etc.\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "data['distance'] = haversine_np(data['item_long'], data['item_lat'], data['buyer_long'], data['buyer_lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the functions that I have written to convert the timestamps to dates\n",
    "\n",
    "# This function splits and considers only the date from a timestamp variable\n",
    "def date(string):\n",
    "    return string.split()[0]\n",
    "\n",
    "# This function is used to convert the string type date object into date type\n",
    "def convert_data(string):\n",
    "    return datetime.strptime(str(string), '%Y-%m-%d').date()\n",
    "\n",
    "# This is used to calculate the total numebr of days taken to deliver the product to the customer from the seller\n",
    "# since the date of order\n",
    "def number_of_days(string):\n",
    "    return (string['delivery_date_modified'] - string['payment_date']).days\n",
    "\n",
    "# This is used to calculate the total numebr of days taken to deliver the product to the carrier from the seller\n",
    "# since the date of order\n",
    "def number_of_handling_days(string):\n",
    "    return (string['acceptance_date'] - string['payment_date']).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0be44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the above defined functions to generate date from the payment_datetime column and storing it in a new column\n",
    "\n",
    "data['payment_date'] = data['payment_datetime'].apply(date)\n",
    "data['payment_date'] = data['payment_date'].apply(convert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f50d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating date from the delivery_date column and storing it in a new column\n",
    "data['delivery_date_modified'] = data['delivery_date'].apply(convert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the total numebr of days taken to deliver the product to the customer from the seller since the date of order and\n",
    "# storing it in a new column\n",
    "data['no_of_days_after_payment'] = data.apply(number_of_days, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec2336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the above defined functions to generate date from the acceptance_scan_timestamp column and storing it in a new column\n",
    "\n",
    "data['acceptance_date'] = data['acceptance_scan_timestamp'].apply(date)\n",
    "data['acceptance_date'] = data['acceptance_date'].apply(convert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6be23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total numebr of days taken to deliver the product to the carrier from the seller since the date of order and\n",
    "# storing it in a new column\n",
    "data['handling_days'] = data.apply(number_of_handling_days, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d861bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the indexes of data points which has a negative handling days period. This is done to remove them considering \n",
    "# they \n",
    "are faulty human inputs.\n",
    "to_drop = data[data['no_of_days_after_payment'] < 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the negative values present in the no_of_days_after_payment column\n",
    "data.drop(index = to_drop, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing the Null values, I remove the columns that are of less importance in predicting the delivery dates\n",
    "data.drop(['item_coord', 'buyer_coord', 'item_lat', 'item_long', 'buyer_lat', 'buyer_long','weight_units','weight','payment_datetime'], axis = 1, inplace = True)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all the rows containing the Null values and storing it in a new dataframe\n",
    "model_data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a4e56",
   "metadata": {},
   "source": [
    "After all the cleaning has been done, I have stored the cleaned dataframe in a new variable called model_data. I have done this as I have more cleaning ideas that I will be implementing before the final eBay ML Challenge submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab147f",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79677cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the histogram of the number of days it takes for the delivery to arrive to the buyer\n",
    "plt.hist(model_data['no_of_days_after_payment'], bins = 200)\n",
    "plt.xlim(0, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2726050",
   "metadata": {},
   "source": [
    "I have plotted the frequency of the days a shipment takes to reach the customer. From the above graph, we can see that, most of the packages arrive by 3 to 4 days. These two days alone cover nearly 50% of all the transactions in our dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a scatter plot of the distance against shipment_method_id\n",
    "sns.scatterplot(x = model_data['shipment_method_id'], y = model_data['distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9f475",
   "metadata": {},
   "source": [
    "From the above graph, we can clearly see that the shipment method id 0 and 1 do the bulk of the shipment when it comes to long distance. For this reason, it is safe to assume these two shipment method id's are part of the airplanes. Although most of the shipments cover a distance of 0 to 4000 Kms, shipemt id's after 15 cover less number of transactions in that range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6383be",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "As this is a dataset obtained from the eBay ML Chanllenge, I will be using the loss function defined by them to calculate the accuracy of my models. The funtion basically penalizes the model with a value of 0.4 for every early delivery and 0.6 if there is a late delivery. Lesser the loss, better the model. The baseline loss score of the current existing model is given by eBay as 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24850dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(preds, actual):\n",
    "    early_loss, late_loss = 0,0 \n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] < actual[i]:\n",
    "            #early shipment\n",
    "            early_loss += actual[i] - preds[i]\n",
    "        elif preds[i] > actual[i]:\n",
    "            #late shipment\n",
    "            late_loss += preds[i] - actual[i]\n",
    "    loss = (1/len(preds)) * (0.4 * (early_loss) + 0.6 * (late_loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d3127",
   "metadata": {},
   "source": [
    "# Train and Test data\n",
    "Before running the models, I have split the data into train set and test set. I have done this using the sklearn library by giving 80% of the data to training and the rest 20% to testing. I have also shuffled the dataset before splitting to make sure that data points present in different parts of the file are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model_data[['b2c_c2c', 'declared_handling_days', 'carrier_average_estimate', 'distance', 'quantity', 'weight_kg',\n",
    "          'handling_days']]\n",
    "y = model_data['no_of_days_after_payment']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe2d43",
   "metadata": {},
   "source": [
    "# Models\n",
    "I have built three models for predicting the delivery dates. All the models produce a slightly better loss value than the baseline of 0.75. The models that I have trained are,\n",
    "1. Multivariate Linear Regression\n",
    "2. MLP Regressor\n",
    "3. XGBoost Ensemble Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec02422",
   "metadata": {},
   "source": [
    "### Multi-variate Linear Regression\n",
    "As the name suggests, more than one variable is used to determine the predictor i.e. delivery date in our case. This is a linear regression model with multiple variables to determine the output. I have used this to check if the combination of variables has a relation with the predicting variable even though the maximum correlation between any one variable with the predicting variable is just 0.46. Another reason for using this model is to find outliers/anomalies if there are any. I have used the LinearRegression function present in the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Linear Regression model using the sklearn library\n",
    "model = LinearRegression()\n",
    "\n",
    "# Training the model with the train dataset\n",
    "model.fit(np.array(x_train), y_train)\n",
    "\n",
    "# Predicting the target values for the test dataset\n",
    "predictions = model.predict(np.array(x_test))\n",
    "\n",
    "# Evaluating the loss of the model using the eBay defined function\n",
    "print('Linear Regression loss:', evaluate_loss([np.floor(x) for x in predictions], np.array(y_test)))\n",
    "print('r2 of the model:', r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5247fb",
   "metadata": {},
   "source": [
    "Although, the r2 of the model is quite low, I am considering this as it has a better loss compared to the baseline score provided by eBay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83fbad8",
   "metadata": {},
   "source": [
    "### MLP Regressor\n",
    "MLP stands for Multi-Layer Perceptron and it is a basic Artificial Neural Network model present in the scikit-learn library. Some of the salient features of the algorithm are that,\n",
    "1. It does not have an activation function in the output layer.\n",
    "2. Squared Error is used as a loss function in the regressor models, whereas, cross-entropy is used in classification models.\n",
    "3. It does not support implementation on the GPU\n",
    "\n",
    "The model is not good for real-time data applications as we can see in the below results. The accuracy of the model is almost the same as the accuracy of the multivariate linear regression model. I have used this model to learn about the basic deep learning models that can be implemented from the scikit-learn library without the support of any GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the MLP Regressor with 5 hidden layers and an activation function\n",
    "model = MLPRegressor(hidden_layer_sizes = (2,3,4,3,2), activation=\"relu\", solver=\"lbfgs\", max_iter=100)\n",
    "\n",
    "# Training the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Generating predictions\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Calculating the loss of our model\n",
    "print('MLP Regressor Loss:', evaluate_loss([np.floor(i) for i in predictions], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c890bad",
   "metadata": {},
   "source": [
    "I have built an MLP model with 5 layers with an activation function 'relu' in each layer. As per the loss function defined by eBay, my model is doing better than the above implemented multi-variate linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad369541",
   "metadata": {},
   "source": [
    "### XGBoost Ensemble Algorithm\n",
    "It stands for Extreme Gradient Boosting and is an ensemble algorithm that is used to boost the decision trees for higher speed and performance/accuracy. There are three main forms of gradient boosting supported by the library, i.e.,\n",
    "1. Gradient Boosting algorithm also called gradient boosting machine including the learning rate.\n",
    "2. Stochastic Gradient Boosting with sub-sampling at the row, column, and column per split levels.\n",
    "3. Regularized Gradient Boosting with both L1 and L2 regularization.\n",
    "\n",
    "I have used the basic gradient boosting algorithm as it provided better results than the others.\n",
    "The main algorithmic features that made me select this model are,\n",
    "1. Sparse Aware implementation with automatic handling of missing data values.\n",
    "2. Block Structure to support the parallelization of tree construction.\n",
    "3. Continued Training so that I can further boost an already fitted model on new data.\n",
    "The XGBoost implementation speed and the model performance have also made it easier for me to use the model for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the variable of float datatype to int datatype\n",
    "a = model_data['package_size'].copy()\n",
    "a = a.astype(int)\n",
    "\n",
    "# Since, XGBoost needs data in a different format, I will be reassinging x and y as per required\n",
    "x = np.column_stack((model_data['b2c_c2c'],model_data['seller_id'], model_data['declared_handling_days'], \n",
    "                    model_data['shipment_method_id'],model_data['shipping_fee'], model_data['carrier_min_estimate'], \n",
    "                    model_data['carrier_max_estimate'], model_data['item_zip'], model_data['buyer_zip'], \n",
    "                    model_data['category_id'], model_data['item_price'], model_data['weight_kg'], model_data['quantity'], a, \n",
    "                    model_data['handling_days']))\n",
    "\n",
    "y = np.array(model_data['no_of_days_after_payment'])\n",
    "\n",
    "# Splitting the data into train and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "# Creating an XGBoost model\n",
    "model = xgb.XGBRegressor(verbosity = 0)\n",
    "\n",
    "# Trainging the dataset\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the values for the test dataset\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Calculating the loss of our model\n",
    "print('XGBoost Loss:', evaluate_loss([np.floor(i) for i in predictions],np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d25fb7",
   "metadata": {},
   "source": [
    "As we can see from the outputs of all the developed models, XGBoost is giving the best results. The results are quite expected as seen from the article written by Purdue university which shows Random Forest Ensemble method to be the best. XGBoost is supposed to be better both in terms of speed and performance. The results are quite fulfilling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a326e",
   "metadata": {},
   "source": [
    "# Learning Curve\n",
    "The project has been a great learning curve for me. I was able to practice the whole data cleaning process on a real-world data set, explain the dataset with visualizations, and build models on top of it. The models that I built were also something new that I learned while going through the project. I plan to enhance the model accuracy by understanding the DeepETA algorithm and working on it in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09806609",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "As I have mentioned before, I have planned to do more data cleaning that includes, excluding the weekends for the number_of_handling_days andno_of_days_after_payment column. I also have to consider the time of order or shipping because if the order is after 5:00 P.M. I should take the next business day for further processing. As for the models, although the developed ones are above the baseline score defined by eBay, I would like to build a Convolutional Neural Network model as defined in the paper  \"DeepETA: A Spatial-Temporal Sequential Neural Network Model for Estimating Time of Arrival in Package Delivery System\" as a continuation for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b252be",
   "metadata": {},
   "source": [
    "# References\n",
    "1. \"Predicting Shipping Time with Machine Learning\", Presented at Massachusets Institute of Technology, 2012.\n",
    "2. \"A Machine Learning Approach to Delivery Time Estimation for Industrial Equipment\", Purdue University.\n",
    "3. \"DeepETA: A Spatial-Temporal Sequential Neural Network Model for Estimating Time of Arrival in Package Delivery System\", Assosciation for the Advancement of Artificial Intelligence, 2019. \n",
    "4. \"Boosting Algorithms for Delivery Time Prediction in Transportation Logistics\", International Conference on Data Mining Workshops, 2020, DOI: 10.1109/ICDMW51313.2020.00043\n",
    "5. \"Predicting the Last Mile: Route-Free Prediction of Parcel Delivery Time with Deep Learning for Smart-City Applications\", Queen's University, Canada, 2020.\n",
    "6. \"Predicting Package Delivery Time For Motorcycles In Nairobi\", Kenya College of Accountancy, 2020, DOI:10.13140/RG.2.2.27105.94567\n",
    "7. Haversine formula - https://stackoverflow.com/questions/29545704/fast-haversine-approximation-python-pandas/29546836#29546836\n",
    "8. Similar work - https://milliemince.github.io/eBay-shipping-predictions/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378e410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
